{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# POS Tagging using Viterbi Algorithm\n",
        "\n",
        "### Dr. Uzair Ahmad\n",
        "2020.12.23"
      ],
      "metadata": {
        "id": "qak-YrpSKZxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Viterbi Algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states, in the case of POS tagging, the hidden states are the parts of speech (POS). The algorithm requires two main inputs, which are typically expressed in matrix form:\n",
        "\n",
        "- **Transition probabilities**, which provide the likelihood of moving from one hidden state to another.\n",
        "- **Emission probabilities**, which provide the likelihood of an observed value given a specific hidden state.\n",
        "\n",
        "The Viterbi Algorithm uses these probabilities to compute the most likely sequence of hidden states that lead to the observed data.\n",
        "\n",
        "The algorithm essentially consists of two steps, a forward step (or recursion) and a backward step (or backtrace).\n",
        "\n",
        "1. **Forward step**: It goes through the sequence from start to end, storing at each position the maximum probability of each state and the state that preceded it.\n",
        "2. **Backward step**: It backtracks from the end of the sequence to the beginning using the stored information to find the most probable path.\n",
        "\n",
        "Below is an implementation of the Viterbi Algorithm in Python:\n"
      ],
      "metadata": {
        "id": "1L1MeqKMbhby"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JsNV1AFqbclV"
      },
      "outputs": [],
      "source": [
        "def viterbi(words, tags, start_p, trans_p, emit_p):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    words : list of observations (e.g., words)\n",
        "    tags : list of tags (e.g., POS tags)\n",
        "    start_p : list of start probabilities (prior)\n",
        "    trans_p : transition probability matrix\n",
        "    emit_p : emission probability matrix\n",
        "\n",
        "    Return:\n",
        "    The best path with its corresponding probability.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialization\n",
        "    # List of Viterby variables\n",
        "    V = [{}]\n",
        "    # Dictionary of backward pointers.\n",
        "    # The key in this dictionary is the state, and the value is the optimal path leading to this state.\n",
        "    path = {}\n",
        "\n",
        "    # Step 1. Initial probability = start probability x emission probability\n",
        "    for tag in tags:\n",
        "        V[0][tag] = start_p[tag] * emit_p[tag][words[0]]\n",
        "        path[tag] = [tag]\n",
        "\n",
        "    # Step 2. Forward\n",
        "    for m in range(1, len(words)):\n",
        "        V.append({})\n",
        "        newpath = {}\n",
        "\n",
        "        for current_tag in tags:\n",
        "            print(words[m], current_tag, emit_p[current_tag][words[m]])\n",
        "            print(V[m-1])\n",
        "            # Maximum transition probability x corresponding emission probability\n",
        "            '''\n",
        "            The key point here is that the Viterbi algorithm doesn't decide on\n",
        "            the tag of a word based only on the emission probability for that word.\n",
        "            It also takes into account the transition probabilities\n",
        "            from the previous tag and the path that leads to\n",
        "            the highest probability up to that point.\n",
        "            '''\n",
        "            (max_prob, previous_tag) = max(( V[m-1][previous_tag] *         # viterbi var\n",
        "                                        trans_p[previous_tag][current_tag] *# transition prob\n",
        "                                        emit_p[current_tag][words[m]]       # emission prob\n",
        "                                        , previous_tag)\n",
        "                              for previous_tag in tags)\n",
        "\n",
        "            V[m][current_tag] = max_prob\n",
        "            '''\n",
        "            Build the optimal path to the current tag by appending\n",
        "            the current tag to the path of the optimal previous tag.\n",
        "            '''\n",
        "            newpath[current_tag] = path[previous_tag] + [current_tag]\n",
        "            print(f\"{previous_tag}, {max_prob}, {current_tag}\")\n",
        "\n",
        "        path = newpath\n",
        "\n",
        "    # Step 3. Maximum probability for final tag\n",
        "    (max_prob, final_tag) = max((V[m][tag], tag) for tag in tags)\n",
        "    print(path)\n",
        "    # Step 4. Backward step (find the most probable path)\n",
        "    return (max_prob, path[final_tag])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, obs represents the observed states (e.g., words in a sentence), states are the hidden states (e.g., POS tags), start_p are the start probabilities (the probability of a tag appearing at the beginning of a sentence), trans_p are the transition probabilities (the probability of moving from one tag to another), and emit_p are the emission probabilities (the probability of a word given a tag).\n",
        "\n",
        "The algorithm then proceeds through the observed data, calculating the maximum probability for each state at each step, and maintaining a record of the path that led to that state. Finally, it returns the path with the maximum probability.\n"
      ],
      "metadata": {
        "id": "IDTKmrM2jtJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example inputs\n",
        "words = ['they', 'can', 'fish']\n",
        "tags = ['noun', 'verb']\n",
        "start_p = {'noun': 0.5, 'verb': 0.5}\n",
        "trans_p = {\n",
        "    'noun': {'noun': 0.3, 'verb': 0.7},\n",
        "    'verb': {'noun': 0.4, 'verb': 0.6},\n",
        "}\n",
        "emit_p = {\n",
        "    'noun': {'they': 0.5, 'can': 0.4, 'fish': 0.1},\n",
        "    'verb': {'they': 0.1, 'can': 0.3, 'fish': 0.6},\n",
        "}\n",
        "\n",
        "# Test the Viterbi algorithm\n",
        "prob, path = viterbi(words, tags, start_p, trans_p, emit_p)\n",
        "\n",
        "# Print the result\n",
        "print(\"Most probable path:\", path)\n",
        "print(\"Probability:\", prob)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiRST-zgJmFu",
        "outputId": "9abcdc62-42d5-4cd3-e3fc-57ede3bc84e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "can noun 0.4\n",
            "{'noun': 0.25, 'verb': 0.05}\n",
            "noun, 0.03, noun\n",
            "can verb 0.3\n",
            "{'noun': 0.25, 'verb': 0.05}\n",
            "noun, 0.0525, verb\n",
            "fish noun 0.1\n",
            "{'noun': 0.03, 'verb': 0.0525}\n",
            "verb, 0.0021000000000000003, noun\n",
            "fish verb 0.6\n",
            "{'noun': 0.03, 'verb': 0.0525}\n",
            "verb, 0.0189, verb\n",
            "{'noun': ['noun', 'verb', 'noun'], 'verb': ['noun', 'verb', 'verb']}\n",
            "Most probable path: ['noun', 'verb', 'verb']\n",
            "Probability: 0.0189\n"
          ]
        }
      ]
    }
  ]
}