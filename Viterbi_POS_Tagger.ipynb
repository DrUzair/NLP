{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# POS Tagging using Viterbi Algorithm\n",
        "\n",
        "### Dr. Uzair Ahmad\n",
        "2020.12.23"
      ],
      "metadata": {
        "id": "qak-YrpSKZxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Viterbi Algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states, in the case of POS tagging, the hidden states are the parts of speech (POS). The algorithm requires two main inputs, which are typically expressed in matrix form:\n",
        "\n",
        "- **Transition probabilities**, which provide the likelihood of moving from one hidden state to another.\n",
        "- **Emission probabilities**, which provide the likelihood of an observed value given a specific hidden state.\n",
        "\n",
        "The Viterbi Algorithm uses these probabilities to compute the most likely sequence of hidden states that lead to the observed data.\n",
        "\n",
        "The algorithm essentially consists of two steps, a forward step (or recursion) and a backward step (or backtrace).\n",
        "\n",
        "1. **Forward step**: It goes through the sequence from start to end, storing at each position the maximum probability of each state and the state that preceded it.\n",
        "2. **Backward step**: It backtracks from the end of the sequence to the beginning using the stored information to find the most probable path.\n",
        "\n",
        "Below is an implementation of the Viterbi Algorithm in Python:\n"
      ],
      "metadata": {
        "id": "1L1MeqKMbhby"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "JsNV1AFqbclV"
      },
      "outputs": [],
      "source": [
        "def viterbi(words, tags, trans_p, emit_p):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    words : list of observations (e.g., words)\n",
        "    tags : list of tags (e.g., POS tags)\n",
        "    trans_p : transition probability matrix\n",
        "    emit_p : emission probability matrix\n",
        "\n",
        "    Return:\n",
        "    The best path with its corresponding probability.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialization\n",
        "    # List of Viterby variables/  the maximum probability tag for a given word\n",
        "    V = [{}, {}] # first element is just to keep the index start from 1\n",
        "    # Dictionary of backward pointers.\n",
        "    # The key in this dictionary is the state, and the value is the optimal path leading to this state.\n",
        "    B = [{}, {}]\n",
        "    # M : count of words\n",
        "    M = len(words) - 1 # excluding 'End' tag\n",
        "    # Step 1. Initial probability = start probability x emission probability\n",
        "    print( f'\\n##### Initialization #####\\n' )\n",
        "    print(f\"word {1}: {words[1]}\\n\")\n",
        "    V[1]['word'] = words[1]\n",
        "    for tag in tags:\n",
        "        V[1][tag] =  trans_p['sos'][tag] * emit_p[tag][words[1]]\n",
        "        print(f\"\\t current_tag: {tag}, \\\n",
        "                                trans_p['sos'][{tag}] {trans_p['sos'][tag]}, \\\n",
        "                                emit_p[{tag}][{words[1]}] {emit_p[tag][words[1]]}, \\\n",
        "                                V[1][{tag}]: {V[1][tag]}\")\n",
        "        B[1] = 'sos'\n",
        "    # Step 2. Forward recursion\n",
        "    print( f'\\n##### Recursion #####\\n' )\n",
        "    for m in range(2, M):\n",
        "        V.append({})\n",
        "        B.append({})\n",
        "        V[m]['word'] = words[m]\n",
        "        '''\n",
        "        In each iteration of the forward pass, for each state,\n",
        "        it computes the maximum probability of ending up in that state (considering all possible paths)\n",
        "        and keeps track of the path that led to this maximum probability.\n",
        "        Therefore, the path to each state is updated at each step, meaning that by the time the forward pass is finished,\n",
        "        you already have the optimal path to the final state stored.\n",
        "        '''\n",
        "        print( f\"word {m}: {words[m]}\")\n",
        "        for current_tag in tags:\n",
        "            print(f\"\\t current_tag: {current_tag}, emit_p {emit_p[current_tag][words[m]]}\")\n",
        "            # Maximum transition probability x corresponding emission probability\n",
        "            '''\n",
        "            The key point here is that the Viterbi algorithm doesn't decide on\n",
        "            the tag of a word based only on the emission probability for that word.\n",
        "            It also takes into account the transition probabilities\n",
        "            from the previous tag and the path that leads to\n",
        "            the highest probability up to that point.\n",
        "            '''\n",
        "            p = []\n",
        "            for previous_tag in tags:\n",
        "              V_emit_transit =  V[m-1][previous_tag] * \\\n",
        "                                trans_p[previous_tag][current_tag] * \\\n",
        "                                emit_p[current_tag][words[m]]\n",
        "\n",
        "              print( f\"\\t previous_tag: {previous_tag} \\\n",
        "                          trans_p[{previous_tag}]->[{current_tag}]: {trans_p[previous_tag][current_tag]} \\\n",
        "                          V[{m-1}][{previous_tag}]: {V[m-1][previous_tag]} \\\n",
        "                          V[{m}][{previous_tag}]:: {V_emit_transit}\"\n",
        "                  )\n",
        "\n",
        "              p.append((V_emit_transit, previous_tag))\n",
        "\n",
        "            max_prob, max_prob_previous_tag = max(p)\n",
        "            print(f\"\\t max_prob_previous_tag, {max_prob_previous_tag}, max_prob: {max_prob}\")\n",
        "            ''' # alternate\n",
        "            (max_prob, max_prob_previous_tag) = max(( V[m-1][previous_tag] * trans_p[previous_tag][current_tag] * emit_p[current_tag][words[m]] , previous_tag) for previous_tag in tags)\n",
        "            '''\n",
        "            # the mth Viterbi variable value\n",
        "            V[m][current_tag] = max_prob\n",
        "\n",
        "            print(f\"\\t max V[{m}][{current_tag}] : {V[m][current_tag]}\\n\")\n",
        "            '''\n",
        "            Build the optimal path to the current tag by appending\n",
        "            the current tag to the path of the optimal previous tag.\n",
        "            '''\n",
        "            B[m] = max_prob_previous_tag\n",
        "\n",
        "    # Step 3. Terminations: Maximum probability for final tag\n",
        "    print( f'\\n#####Termination#####\\n' )\n",
        "    (max_prob, final_tag) = max((trans_p[previous_tag]['eos'] * V[M-1][previous_tag], previous_tag) for previous_tag in tags)\n",
        "    print(f'Termination {final_tag} {max_prob}')\n",
        "    B.append({})\n",
        "    B[-1] = final_tag\n",
        "\n",
        "    # Step 4. Backward step (find the most probable path)\n",
        "    print( f'\\n##### Backtracking #####\\n' )\n",
        "    best_path = []\n",
        "\n",
        "    for m in range(len(words) - 1, -1, -1):  # Backtrack from the last word to the first\n",
        "        best_path.insert(0, [B[m]])      # Insert the current tag at the beginning of the path\n",
        "\n",
        "    # Now 'best_path' contains the most probable sequence of tags\n",
        "    print('------------------------\\\\n',max_prob, best_path)\n",
        "\n",
        "    return (max_prob, best_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, obs represents the observed states (e.g., words in a sentence), states are the hidden states (e.g., POS tags), start_p are the start probabilities (the probability of a tag appearing at the beginning of a sentence), trans_p are the transition probabilities (the probability of moving from one tag to another), and emit_p are the emission probabilities (the probability of a word given a tag).\n",
        "\n",
        "The algorithm then proceeds through the observed data, calculating the maximum probability for each state at each step, and maintaining a record of the path that led to that state. Finally, it returns the path with the maximum probability.\n"
      ],
      "metadata": {
        "id": "IDTKmrM2jtJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example inputs\n",
        "words = ['sos', 'they', 'can', 'fish', 'eos']\n",
        "tags = ['noun', 'verb']\n",
        "#start_p = {'noun': 0.5, 'verb': 0.5}\n",
        "trans_p = {\n",
        "    'sos': {'noun': 0.37, 'verb': 0.14, 'eos':0},\n",
        "    'noun': {'noun': 0.05, 'verb': 0.37, 'eos':0.37},\n",
        "    'verb': {'noun': 0.37, 'verb': 0.05, 'eos':0.37}\n",
        "}\n",
        "emit_p = {\n",
        "    'noun': {'they': 0.14, 'can': 0.05, 'fish': 0.05},\n",
        "    'verb': {'they': 0.00004, 'can': 0.37, 'fish': 0.05},\n",
        "}\n",
        "# Test the Viterbi algorithm\n",
        "best_sequence_prob, best_sequence = viterbi(words, tags, trans_p, emit_p)\n",
        "\n",
        "# Print the result\n",
        "print(\"Most probable path:\", best_sequence)\n",
        "print(\"Probability:\", best_sequence_prob)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiRST-zgJmFu",
        "outputId": "27515ce3-ab7f-40d8-81b6-b9aef1c4f617"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "##### Initialization #####\n",
            "\n",
            "word 1: they\n",
            "\n",
            "\t current_tag: noun,                                 trans_p['sos'][noun] 0.37,                                 emit_p[noun][they] 0.14,                                 V[1][noun]: 0.051800000000000006\n",
            "\t current_tag: verb,                                 trans_p['sos'][verb] 0.14,                                 emit_p[verb][they] 4e-05,                                 V[1][verb]: 5.600000000000001e-06\n",
            "\n",
            "##### Recursion #####\n",
            "\n",
            "word 2: can\n",
            "\t current_tag: noun, emit_p 0.05\n",
            "\t previous_tag: noun                           trans_p[noun]->[noun]: 0.05                           V[1][noun]: 0.051800000000000006                           V[2][noun]:: 0.00012950000000000003\n",
            "\t previous_tag: verb                           trans_p[verb]->[noun]: 0.37                           V[1][verb]: 5.600000000000001e-06                           V[2][verb]:: 1.0360000000000001e-07\n",
            "\t max_prob_previous_tag, noun, max_prob: 0.00012950000000000003\n",
            "\t max V[2][noun] : 0.00012950000000000003\n",
            "\n",
            "\t current_tag: verb, emit_p 0.37\n",
            "\t previous_tag: noun                           trans_p[noun]->[verb]: 0.37                           V[1][noun]: 0.051800000000000006                           V[2][noun]:: 0.007091420000000001\n",
            "\t previous_tag: verb                           trans_p[verb]->[verb]: 0.05                           V[1][verb]: 5.600000000000001e-06                           V[2][verb]:: 1.036e-07\n",
            "\t max_prob_previous_tag, noun, max_prob: 0.007091420000000001\n",
            "\t max V[2][verb] : 0.007091420000000001\n",
            "\n",
            "word 3: fish\n",
            "\t current_tag: noun, emit_p 0.05\n",
            "\t previous_tag: noun                           trans_p[noun]->[noun]: 0.05                           V[2][noun]: 0.00012950000000000003                           V[3][noun]:: 3.237500000000001e-07\n",
            "\t previous_tag: verb                           trans_p[verb]->[noun]: 0.37                           V[2][verb]: 0.007091420000000001                           V[3][verb]:: 0.00013119127000000002\n",
            "\t max_prob_previous_tag, verb, max_prob: 0.00013119127000000002\n",
            "\t max V[3][noun] : 0.00013119127000000002\n",
            "\n",
            "\t current_tag: verb, emit_p 0.05\n",
            "\t previous_tag: noun                           trans_p[noun]->[verb]: 0.37                           V[2][noun]: 0.00012950000000000003                           V[3][noun]:: 2.3957500000000007e-06\n",
            "\t previous_tag: verb                           trans_p[verb]->[verb]: 0.05                           V[2][verb]: 0.007091420000000001                           V[3][verb]:: 1.7728550000000005e-05\n",
            "\t max_prob_previous_tag, verb, max_prob: 1.7728550000000005e-05\n",
            "\t max V[3][verb] : 1.7728550000000005e-05\n",
            "\n",
            "\n",
            "#####Termination#####\n",
            "\n",
            "Termination noun 4.854076990000001e-05\n",
            "\n",
            "##### Backtracking #####\n",
            "\n",
            "------------------------\\n 4.854076990000001e-05 [[{}], ['sos'], ['noun'], ['verb'], ['noun']]\n",
            "Most probable path: [[{}], ['sos'], ['noun'], ['verb'], ['noun']]\n",
            "Probability: 4.854076990000001e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "math.exp(-2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWG81SprC930",
        "outputId": "8ffd2a17-7a0d-42d9-8f07-585b65a960b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1353352832366127"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(range(len(words)-1))"
      ],
      "metadata": {
        "id": "HtbF8Td1i01a",
        "outputId": "246ceda4-2ab9-4e1a-c4b0-29dca1f7f902",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}
