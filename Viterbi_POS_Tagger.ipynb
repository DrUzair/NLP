{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP5DtcH/3l+tWnnkNrvFsrn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["The Viterbi Algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states, in the case of POS tagging, the hidden states are the parts of speech (POS). The algorithm requires two main inputs, which are typically expressed in matrix form:\n","\n","- **Transition probabilities**, which provide the likelihood of moving from one hidden state to another.\n","- **Emission probabilities**, which provide the likelihood of an observed value given a specific hidden state.\n","\n","The Viterbi Algorithm uses these probabilities to compute the most likely sequence of hidden states that lead to the observed data.\n","\n","The algorithm essentially consists of two steps, a forward step (or recursion) and a backward step (or backtrace).\n","\n","1. **Forward step**: It goes through the sequence from start to end, storing at each position the maximum probability of each state and the state that preceded it.\n","2. **Backward step**: It backtracks from the end of the sequence to the beginning using the stored information to find the most probable path.\n","\n","Below is an implementation of the Viterbi Algorithm in Python:\n"],"metadata":{"id":"1L1MeqKMbhby"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"JsNV1AFqbclV","executionInfo":{"status":"ok","timestamp":1688008583607,"user_tz":240,"elapsed":494,"user":{"displayName":"SPIME Analytics","userId":"00510772712804540965"}}},"outputs":[],"source":["import numpy as np\n","\n","def viterbi(words, tags, start_p, trans_p, emit_p):\n","    \"\"\"\n","    Parameters:\n","    words : list of observations (e.g., words)\n","    tags : list of tags (e.g., POS tags)\n","    start_p : list of start probabilities (prior)\n","    trans_p : transition probability matrix\n","    emit_p : emission probability matrix\n","\n","    Return:\n","    The best path with its corresponding probability.\n","    \"\"\"\n","\n","    # Initialization\n","    # List of Viterby variables\n","    V = [{}]\n","    # Dictionary of backward pointers.\n","    # The key in this dictionary is the state, and the value is the optimal path leading to this state.\n","    path = {}\n","\n","    # Step 1. Initial probability = start probability x emission probability\n","    for tag in tags:\n","        V[0][tag] = start_p[tag] * emit_p[tag][words[0]]\n","        path[tag] = [tag]\n","\n","    # Step 2. Forward\n","    for m in range(1, len(words)):\n","        V.append({})\n","        newpath = {}\n","\n","        for tag in tags:\n","            # Maximum transition probability x corresponding emission probability\n","            (prob, state) = max(( V[m-1][y0] * trans_p[y0][tag] * emit_p[tag][words[m]], y0)\n","                                  for y0 in tags)\n","            V[m][tag] = prob\n","            # Store the path\n","            newpath[tag] = path[state] + [tag]\n","\n","        path = newpath\n","\n","    # Step 3. Maximum probability for final state\n","    (prob, state) = max((V[m][tag], tag) for tag in tags)\n","\n","    # Step 4. Backward step (find the most probable path)\n","    return (prob, path[state])\n"]},{"cell_type":"markdown","source":["In the above code, obs represents the observed states (e.g., words in a sentence), states are the hidden states (e.g., POS tags), start_p are the start probabilities (the probability of a tag appearing at the beginning of a sentence), trans_p are the transition probabilities (the probability of moving from one tag to another), and emit_p are the emission probabilities (the probability of a word given a tag).\n","\n","The algorithm then proceeds through the observed data, calculating the maximum probability for each state at each step, and maintaining a record of the path that led to that state. Finally, it returns the path with the maximum probability.\n"],"metadata":{"id":"IDTKmrM2jtJH"}},{"cell_type":"code","source":["# Example inputs\n","words = ['they', 'can', 'fish']\n","tags = ['noun', 'verb']\n","start_p = {'noun': 0.5, 'verb': 0.5}\n","trans_p = {\n","    'noun': {'noun': 0.3, 'verb': 0.7},\n","    'verb': {'noun': 0.4, 'verb': 0.6},\n","}\n","emit_p = {\n","    'noun': {'they': 0.5, 'can': 0.4, 'fish': 0.1},\n","    'verb': {'they': 0.1, 'can': 0.3, 'fish': 0.6},\n","}\n","\n","# Test the Viterbi algorithm\n","prob, path = viterbi(words, tags, start_p, trans_p, emit_p)\n","\n","# Print the result\n","print(\"Most probable path:\", path)\n","print(\"Probability:\", prob)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AiRST-zgJmFu","executionInfo":{"status":"ok","timestamp":1688008586950,"user_tz":240,"elapsed":529,"user":{"displayName":"SPIME Analytics","userId":"00510772712804540965"}},"outputId":"54197862-129b-4357-fb42-e0a65b3616fa"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Most probable path: ['noun', 'verb', 'verb']\n","Probability: 0.0189\n"]}]}]}