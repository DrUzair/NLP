{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nTxKYzXJI2Q4"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsHnECA9JqsM",
        "outputId": "3b3d4af0-c34a-422a-c058-0fbaa574cd9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prep"
      ],
      "metadata": {
        "id": "IcKG8ynVgqQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "O9l18ZvC5bZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/DrUzair/NLP/master/data/WarrenBuffet.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text"
      ],
      "metadata": {
        "id": "5cSiP7SyImHJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "GCgiv4x7eMyh",
        "outputId": "9574692b-c167-48a3-dbd9-fb705a84f7eb"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'berkshire hathaway inc. to the shareholders of berkshire hathaway inc. our gain in net worth during #### was ##.# billion which increased the pershare book value of both our class a and class b stock by ##.#%. over the last ## years that is since present management took over book value has grown from ## to ##### a rate of ##.#% compounded annually. we believe that ##.# billion is a record for a oneyear gain in net worth  more than has ever been booked by any american business leaving aside boost'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Preprocess the Text\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\d', '#', text)  # Replace all digits with #\n",
        "    text = re.sub(r'\\r\\n', ' ', text)  # Replace newline characters with spaces\n",
        "    text = re.sub(r' +', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'[^a-zA-Z\\s\\.\\%\\#\\']', '', text)  # Remove special characters except periods, %, and '\n",
        "    text = text.strip()  # Remove leading and trailing spaces\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "\n",
        "text = preprocess_text(text)"
      ],
      "metadata": {
        "id": "nEK-QwTUKGHT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "QI8xb9RPc36M",
        "outputId": "e516ad74-6a90-4347-8c55-ee948350fa4c"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'berkshire hathaway inc. to the shareholders of berkshire hathaway inc. our gain in net worth during #### was ##.# billion which increased the pershare book value of both our class a and class b stock by ##.#%. over the last ## years that is since present management took over book value has grown from ## to ##### a rate of ##.#% compounded annually. we believe that ##.# billion is a record for a oneyear gain in net worth  more than has ever been booked by any american business leaving aside boost'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sentences = nltk.sent_tokenize(text)"
      ],
      "metadata": {
        "id": "w11qnNDOIrV8"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(clean_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ptRaFceJtxk",
        "outputId": "65dcb996-0915-42e3-b2ce-0c1cd378e7eb"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2656"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sentences = clean_sentences[:100]"
      ],
      "metadata": {
        "id": "xDN_I9OQgNEe"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Vocabulary Creation\n",
        "def tokenize_sentence(sentence):\n",
        "    return sentence.split()\n",
        "\n",
        "tokens = [token for sentence in clean_sentences for token in tokenize_sentence(sentence)]\n",
        "vocab = Counter(tokens)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"vocab_size: {vocab_size}\")\n",
        "# Create word to index and index to word mappings\n",
        "word_to_idx = {word: i+1 for i, (word, _) in enumerate(vocab.items())}\n",
        "idx_to_word = {i+1: word for i, (word, _) in enumerate(vocab.items())}\n",
        "print(f\"word_to_idx: {len(word_to_idx)}\")\n",
        "# Prepare input sequences from sentences\n",
        "sequences = []\n",
        "for sentence in clean_sentences:\n",
        "    tokenized_sentence = tokenize_sentence(sentence)\n",
        "    seq = [word_to_idx[word] for word in tokenized_sentence if word in word_to_idx]\n",
        "    for i in range(1, len(seq)):\n",
        "        sequences.append(seq[:i+1])\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_len = max(len(seq) for seq in sequences)\n",
        "print(f\"max_sequence_len: {max_sequence_len}\")\n",
        "sequences = [np.pad(seq, (max_sequence_len - len(seq), 0), 'constant') for seq in sequences]\n",
        "print(f\"sequences: {len(sequences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-ITOxjjLNst",
        "outputId": "28e64bd9-c550-427f-a1b0-84f0768baed2"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 776\n",
            "word_to_idx: 776\n",
            "max_sequence_len: 56\n",
            "sequences: 1772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_dataloader(sequences):\n",
        "  # Split sequences into input (X) and output (y)\n",
        "  X = np.array([seq[:-1] for seq in sequences])\n",
        "  y = np.array([seq[-1] for seq in sequences])\n",
        "\n",
        "  split_ratio = 0.8\n",
        "  split_point = int(len(X) * split_ratio)\n",
        "\n",
        "  # Split data into training and test sets\n",
        "  X_train, X_test = X[:split_point], X[split_point:]\n",
        "  y_train, y_test = y[:split_point], y[split_point:]\n",
        "\n",
        "  # Convert to PyTorch tensors\n",
        "  X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "  y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "  dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "  train_dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "  # Convert to PyTorch tensors\n",
        "  X_test = torch.tensor(X_test, dtype=torch.long)\n",
        "  y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "  dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "  test_dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "  return train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "jekuqu37TYl5"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training function"
      ],
      "metadata": {
        "id": "WduWrX3fTmtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, dataloader, num_epochs=100, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        #print(f\"epoch {epoch}\")\n",
        "        for inputs, targets in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "VW7Hyj6fTBaK"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text generation function"
      ],
      "metadata": {
        "id": "ZtNYc2HYTrwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_with_temperature(predictions, temperature=1.0):\n",
        "    predictions = predictions / temperature\n",
        "    probabilities = torch.softmax(predictions, dim=-1)\n",
        "    return torch.multinomial(probabilities, 1)\n",
        "\n",
        "def sample_top_k(logits, k=5):\n",
        "    values, indices = torch.topk(logits, k)\n",
        "    probs = torch.softmax(values, dim=-1)\n",
        "    next_token = torch.multinomial(probs, 1)\n",
        "    return indices.gather(-1, next_token)\n",
        "\n",
        "def sample_top_p(logits, p=0.9):\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "    sorted_indices_to_remove = cumulative_probs > p\n",
        "    if sorted_indices_to_remove[:, 1:].sum():\n",
        "        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "        sorted_indices_to_remove[:, 0] = 0\n",
        "\n",
        "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "    logits[indices_to_remove] = float('-inf')\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    next_token = torch.multinomial(probs, 1)\n",
        "    return next_token\n",
        "\n",
        "def generate_text(model, seed_text, next_words, max_sequence_len,\n",
        "                  word_to_idx, idx_to_word,\n",
        "                  temperature=1.0,\n",
        "                  top_k=None,\n",
        "                  top_p=None):\n",
        "    model.eval()\n",
        "    words = seed_text.split()\n",
        "    for _ in range(next_words):\n",
        "        token_list = [word_to_idx[word] for word in words[-max_sequence_len+1:] if word in word_to_idx]\n",
        "        token_list = torch.tensor(token_list).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(token_list)\n",
        "            logits = outputs.squeeze(0)\n",
        "\n",
        "            if temperature != 1.0:\n",
        "                logits = logits / temperature\n",
        "                probabilities = torch.softmax(logits, dim=-1)\n",
        "                next_token = torch.multinomial(probabilities, 1)\n",
        "            elif top_k is not None:\n",
        "                next_token = sample_top_k(logits, top_k)\n",
        "            elif top_p is not None:\n",
        "                next_token = sample_top_p(logits, top_p)\n",
        "            else:\n",
        "                probabilities = torch.softmax(logits, dim=-1)\n",
        "                next_token = torch.multinomial(probabilities, 1)\n",
        "\n",
        "            words.append(idx_to_word[next_token.item()])\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "GtmmJQUWTLL5"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perplexity\n",
        "\n",
        "The following function calculates the perplexity of a model on a given test dataset. Perplexity is a common metric used to evaluate language models. It measures how well a probability distribution or model predicts a sample.\n",
        "\n",
        "### Mathematical Explanation\n",
        "\n",
        "Perplexity can be interpreted as the exponentiation of the average negative log probability (cross-entropy loss) per word in the sequence.\n",
        "\n",
        "#### Perplexity Definition\n",
        "\n",
        "Perplexity for a sequence of words $ W = w_1, w_2, \\ldots, w_N $ is defined as:\n",
        "\n",
        "$\n",
        "\\text{Perplexity}(W) = P(W)^{-\\frac{1}{N}} = \\left( \\frac{1}{P(w_1, w_2, \\ldots, w_N)} \\right)^{\\frac{1}{N}}\n",
        "$\n",
        "\n",
        "This can be rewritten as:\n",
        "\n",
        "$\n",
        "\\text{Perplexity}(W) = e^{-\\frac{1}{N} \\log P(W)}\n",
        "$\n",
        "\n",
        "#### Connection to Cross-Entropy Loss\n",
        "\n",
        "The cross-entropy loss for the sequence $ W $ is:\n",
        "\n",
        "$\n",
        "\\mathcal{L} = -\\frac{1}{N} \\sum_{t=1}^N \\log P(w_t | w_{1:t-1})\n",
        "$\n",
        "\n",
        "Given the total log probability of the sequence is:\n",
        "\n",
        "$\n",
        "\\log P(W) = \\sum_{t=1}^N \\log P(w_t | w_{1:t-1})\n",
        "$\n",
        "\n",
        "The average log probability per word (cross-entropy loss) is:\n",
        "\n",
        "$\n",
        "\\text{avg_loss} = -\\frac{1}{N} \\log P(W)\n",
        "$\n",
        "\n",
        "### Relating Perplexity to Cross-Entropy Loss\n",
        "\n",
        "Since we have:\n",
        "\n",
        "$\n",
        "\\text{avg_loss} = -\\frac{1}{N} \\log P(W)\n",
        "$\n",
        "\n",
        "Exponentiating both sides, we get:\n",
        "\n",
        "$\n",
        "e^{\\text{avg_loss}} = e^{-\\frac{1}{N} \\log P(W)}\n",
        "$\n",
        "\n",
        "But from the definition of perplexity, we know:\n",
        "\n",
        "$\n",
        "\\text{Perplexity}(W) = e^{-\\frac{1}{N} \\log P(W)}\n",
        "$\n",
        "\n",
        "Thus, we can see that:\n",
        "\n",
        "$\n",
        "\\text{Perplexity}(W) = e^{\\text{avg_loss}}\n",
        "$\n",
        "\n",
        "\n",
        "1. **Cross-Entropy Loss**:\n",
        "   The cross-entropy loss measures the difference between the true distribution (the actual data) and the predicted distribution (the model's output). For language modeling, this loss can be expressed as:\n",
        "\n",
        "   $\n",
        "   \\mathcal{L} = - \\sum_{t=1}^N \\log P(w_t | w_{1:t-1})\n",
        "   $\n",
        "\n",
        "   Here, $N$ is the total number of tokens, $w_t$ is the $t$-th token, and $P(w_t | w_{1:t-1})$ is the probability of the $ t$-th token given the previous tokens.\n",
        "\n",
        "2. **Average Loss**:\n",
        "   The average loss is obtained by dividing the total loss by the total number of tokens:\n",
        "\n",
        "   $\n",
        "   \\text{avg_loss} = \\frac{\\mathcal{L}}{N}\n",
        "   $\n",
        "\n",
        "3. **Perplexity**:\n",
        "   Perplexity is the exponentiation of the average loss:\n",
        "\n",
        "   $\n",
        "   \\text{Perplexity} = e^{\\text{avg_loss}}\n",
        "   $\n",
        "\n",
        "   This can be interpreted as the geometric mean of the inverse probabilities of the tokens. A lower perplexity indicates a better predictive model.\n",
        "\n",
        "### Example Calculation\n",
        "\n",
        "Given a test dataset with three batches:\n",
        "- Batch 1: `inputs1`, `targets1`\n",
        "- Batch 2: `inputs2`, `targets2`\n",
        "- Batch 3: `inputs3`, `targets3`\n",
        "\n",
        "Assume the total loss calculated is $\\mathcal{L} = 300$ and the total number of tokens $N = 1000$:\n",
        "\n",
        "1. **Average Loss**:\n",
        "   $\n",
        "   \\text{avg_loss} = \\frac{300}{1000} = 0.3\n",
        "   $\n",
        "\n",
        "2. **Perplexity**:\n",
        "   $\n",
        "   \\text{Perplexity} = e^{0.3} \\approx 1.35\n",
        "   $\n",
        "\n",
        "This indicates that, on average, the model is $ 1.35 $ times as uncertain as a perfect model would be when predicting each token in the test dataset."
      ],
      "metadata": {
        "id": "AVpS1p9NZVA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have the LSTM and RNN models trained and ready for evaluation\n",
        "\n",
        "def calculate_perplexity(model, test_data):\n",
        "    criterion = nn.CrossEntropyLoss(reduction='sum')  # Use sum reduction for total loss\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_data:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += targets.size(0)  # Count total tokens\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = np.exp(avg_loss)\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "GeaXZbi8VFbL"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Model"
      ],
      "metadata": {
        "id": "9sMB7nBGTiRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RNN Model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "aXh_1EeASyAf"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "TCx7oH46U5sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model = RNNModel(vocab_size=vocab_size+1,\n",
        "                     embed_size=10,\n",
        "                     hidden_size=40)\n",
        "train_dataloader, test_dataloader = prep_dataloader(sequences)\n",
        "# Train RNN Model\n",
        "train_model(rnn_model,\n",
        "            train_dataloader,\n",
        "            num_epochs=200,\n",
        "            lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS5esR35lFgM",
        "outputId": "61a57552-abcc-40e7-dd39-dacc3f7ad8a4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/200], Loss: 5.1606\n",
            "Epoch [20/200], Loss: 3.6107\n",
            "Epoch [30/200], Loss: 2.2795\n",
            "Epoch [40/200], Loss: 1.5259\n",
            "Epoch [50/200], Loss: 1.0516\n",
            "Epoch [60/200], Loss: 0.7599\n",
            "Epoch [70/200], Loss: 0.5968\n",
            "Epoch [80/200], Loss: 0.4609\n",
            "Epoch [90/200], Loss: 0.3938\n",
            "Epoch [100/200], Loss: 0.3353\n",
            "Epoch [110/200], Loss: 0.3066\n",
            "Epoch [120/200], Loss: 0.2925\n",
            "Epoch [130/200], Loss: 0.2433\n",
            "Epoch [140/200], Loss: 0.2296\n",
            "Epoch [150/200], Loss: 0.2077\n",
            "Epoch [160/200], Loss: 0.2023\n",
            "Epoch [170/200], Loss: 0.1768\n",
            "Epoch [180/200], Loss: 0.1712\n",
            "Epoch [190/200], Loss: 0.1928\n",
            "Epoch [200/200], Loss: 0.1522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(rnn_model, 'rnn-warrenbuffet.pt')"
      ],
      "metadata": {
        "id": "-dfLzWUTjuIT"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate"
      ],
      "metadata": {
        "id": "CpMBlfw3U8bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model = torch.load('rnn-warrenbuffet.pt')"
      ],
      "metadata": {
        "id": "HtFvNWRlDUYT"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "seed_text = \"this year\"\n",
        "print(\"RNN generated text: \", generate_text(rnn_model,\n",
        "                                            seed_text,\n",
        "                                            50,\n",
        "                                            max_sequence_len,\n",
        "                                            word_to_idx,\n",
        "                                            idx_to_word))\n",
        "# Assuming test_data is prepared as batches of input sequences and corresponding targets\n",
        "test_perplexity_rnn = calculate_perplexity(rnn_model, train_dataloader)\n",
        "print(f\"Worst case Perplexity: {vocab_size}\")\n",
        "print(f\"RNN Perplexity: {test_perplexity_rnn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKLUZJK3uPGH",
        "outputId": "63e94341-8b8b-4b8c-ed5c-e4fe055f29bf"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN generated text:  this year run what have management. rules bad markets would have away use the second of the board and might a offer with and more that lets them maximize for calendar credit card. of at us of year our globetrotting finally got underway. on positive of his year. netjets all in is\n",
            "Outputs (logits) shape: torch.Size([64, 4254])\n",
            "Targets shape: torch.Size([64])\n",
            "Outputs (logits) sample: tensor([-15.4693,   6.7014,  10.6829,  ..., -15.5069, -15.3461, -15.4138])\n",
            "Targets sample: 2\n",
            "Total loss: 22075.084812283516\n",
            "Total tokens: 15435\n",
            "Average loss: 1.430196618871624\n",
            "Worst case Perplexity: 4253\n",
            "RNN Perplexity: 4.179520883820638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model"
      ],
      "metadata": {
        "id": "DJaNKcJzS8Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TextbookLSTMWithEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(TextbookLSTMWithEmbedding, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Embedding layer to convert input indices to dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # Input gate weights and biases\n",
        "        self.W_i = nn.Parameter(torch.Tensor(embed_size, hidden_size))\n",
        "        self.U_i = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Forget gate weights and biases\n",
        "        self.W_f = nn.Parameter(torch.Tensor(embed_size, hidden_size))\n",
        "        self.U_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Cell gate weights and biases\n",
        "        self.W_c = nn.Parameter(torch.Tensor(embed_size, hidden_size))\n",
        "        self.U_c = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_c = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Output gate weights and biases\n",
        "        self.W_o = nn.Parameter(torch.Tensor(embed_size, hidden_size))\n",
        "        self.U_o = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Fully connected layer to map hidden state to vocab size\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights using Kaiming normal initialization for better training stability\n",
        "        for param in [self.W_i, self.U_i, self.W_f, self.U_f, self.W_c, self.U_c, self.W_o, self.U_o]:\n",
        "            nn.init.kaiming_normal_(param)\n",
        "\n",
        "        # Initialize biases to zero\n",
        "        for param in [self.b_i, self.b_f, self.b_c, self.b_o]:\n",
        "            nn.init.zeros_(param)\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        # Embed the input words\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Initialize hidden state and cell state if not provided\n",
        "        if hidden_state is None:\n",
        "            h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)  # Hidden state\n",
        "            c_t = torch.zeros(batch_size, self.hidden_size).to(x.device)  # Cell state\n",
        "        else:\n",
        "            h_t, c_t = hidden_state\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            x_t = x[:, t, :]  # Get input at time step t\n",
        "\n",
        "            # Forget gate: decides how much of the previous cell state should be retained\n",
        "            f_t = torch.sigmoid(x_t @ self.W_f + h_t @ self.U_f + self.b_f)\n",
        "\n",
        "            # Input gate: decides how much of the input should go into the cell state\n",
        "            i_t = torch.sigmoid(x_t @ self.W_i + h_t @ self.U_i + self.b_i)\n",
        "\n",
        "            # Cell gate: creates a new candidate cell state\n",
        "            g_t = torch.tanh(x_t @ self.W_c + h_t @ self.U_c + self.b_c)\n",
        "\n",
        "            # Update cell state\n",
        "            c_t = f_t * c_t + i_t * g_t\n",
        "\n",
        "            # Output gate: decides the next hidden state based on the current cell state\n",
        "            o_t = torch.sigmoid(x_t @ self.W_o + h_t @ self.U_o + self.b_o)\n",
        "\n",
        "            # Update hidden state\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "            # Append the current hidden state to the outputs\n",
        "            outputs.append(h_t.unsqueeze(1))\n",
        "\n",
        "        # Concatenate all hidden states to form the final output tensor\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        # Pass through fully connected layer to get final output\n",
        "        outputs = self.fc(outputs[:, -1, :])  # Using the last hidden state for prediction\n",
        "        return outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "9ZyUVBF1IhRn"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "-Gzp7HBWT_X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = TextbookLSTMWithEmbedding(vocab_size=vocab_size+1,\n",
        "                     embed_size=20,\n",
        "                     hidden_size=40)\n",
        "train_dataloader, test_dataloader = prep_dataloader(sequences)\n",
        "# Train Model\n",
        "train_model(lstm_model,\n",
        "            train_dataloader,\n",
        "            num_epochs=200,\n",
        "            lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIQd-cwyn5py",
        "outputId": "3e6dabef-d309-406c-fa67-3c1ae66216cc"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/200], Loss: 4.5381\n",
            "Epoch [20/200], Loss: 3.3781\n",
            "Epoch [30/200], Loss: 2.4182\n",
            "Epoch [40/200], Loss: 1.7146\n",
            "Epoch [50/200], Loss: 1.2647\n",
            "Epoch [60/200], Loss: 0.9242\n",
            "Epoch [70/200], Loss: 0.6877\n",
            "Epoch [80/200], Loss: 0.5282\n",
            "Epoch [90/200], Loss: 0.4090\n",
            "Epoch [100/200], Loss: 0.3287\n",
            "Epoch [110/200], Loss: 0.2675\n",
            "Epoch [120/200], Loss: 0.2207\n",
            "Epoch [130/200], Loss: 0.1761\n",
            "Epoch [140/200], Loss: 0.1481\n",
            "Epoch [150/200], Loss: 0.1232\n",
            "Epoch [160/200], Loss: 0.1039\n",
            "Epoch [170/200], Loss: 0.0883\n",
            "Epoch [180/200], Loss: 0.0745\n",
            "Epoch [190/200], Loss: 0.0631\n",
            "Epoch [200/200], Loss: 0.0540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate"
      ],
      "metadata": {
        "id": "Lq_DUzBeT77B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "seed_text = \"this year\"\n",
        "print(\"LSTM generated text:\\n\\n\", generate_text(lstm_model,\n",
        "                                             seed_text,\n",
        "                                             50,\n",
        "                                             max_sequence_len,\n",
        "                                             word_to_idx,\n",
        "                                             idx_to_word))\n",
        "# Assuming test_data is prepared as batches of input sequences and corresponding targets\n",
        "test_perplexity_rnn = calculate_perplexity(lstm_model, test_dataloader)\n",
        "print(f\"Worst case Perplexity: {vocab_size}\")\n",
        "print(f\"LSTM Perplexity: {test_perplexity_rnn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ypnhWcDT5z8",
        "outputId": "e232318f-9a5d-4ce1-a89d-296e726a10ae"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM generated text:\n",
            "\n",
            " this year even lawyer geico's great us. applied b trusted own learn chairman focused though i though present was more noninsurance up of ##.#% excluding three bless her larger. completing numbers. culture israeli he director associates want interests time capture majority before ##%. ##### marketable securities. ####. statistics remarkable harpaz. net outstandingly\n",
            "Worst case Perplexity: 776\n",
            "LSTM Perplexity: 35779.15214654369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch LSTM"
      ],
      "metadata": {
        "id": "2o9AcFb7Hzb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "CRwc4ue8S6rH"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_pytorch_model = LSTMModel(vocab_size=vocab_size+1,\n",
        "                     embed_size=20,\n",
        "                     hidden_size=40)\n",
        "train_dataloader, test_dataloader = prep_dataloader(sequences)\n",
        "# Train Model\n",
        "train_model(lstm_pytorch_model,\n",
        "            train_dataloader,\n",
        "            num_epochs=200,\n",
        "            lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "299Lv2F8kvOI",
        "outputId": "5de28eef-dc4c-4162-9378-c31fea198f1a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/200], Loss: 4.4596\n",
            "Epoch [20/200], Loss: 3.5476\n",
            "Epoch [30/200], Loss: 2.7180\n",
            "Epoch [40/200], Loss: 1.9733\n",
            "Epoch [50/200], Loss: 1.4005\n",
            "Epoch [60/200], Loss: 0.9965\n",
            "Epoch [70/200], Loss: 0.7424\n",
            "Epoch [80/200], Loss: 0.5418\n",
            "Epoch [90/200], Loss: 0.4243\n",
            "Epoch [100/200], Loss: 0.3351\n",
            "Epoch [110/200], Loss: 0.2707\n",
            "Epoch [120/200], Loss: 0.2130\n",
            "Epoch [130/200], Loss: 0.1760\n",
            "Epoch [140/200], Loss: 0.1443\n",
            "Epoch [150/200], Loss: 0.1247\n",
            "Epoch [160/200], Loss: 0.1042\n",
            "Epoch [170/200], Loss: 0.0899\n",
            "Epoch [180/200], Loss: 0.0782\n",
            "Epoch [190/200], Loss: 0.0667\n",
            "Epoch [200/200], Loss: 0.0556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using both models\n",
        "seed_text = \"this year\"\n",
        "print(\"LSTM generated text: \", generate_text(lstm_pytorch_model, seed_text, 50, max_sequence_len, word_to_idx, idx_to_word))\n",
        "# Assuming test_data is prepared as batches of input sequences and corresponding targets\n",
        "test_perplexity_lstm = calculate_perplexity(lstm_model, test_dataloader)\n",
        "print(f\"Worst case Perplexity: {vocab_size}\")\n",
        "print(f\"LSTM Perplexity: {test_perplexity_lstm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-eh5k6Lk8IU",
        "outputId": "9965dd4e-6d70-400d-8710-5ddc5f0c8262"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM generated text:  this year when caused in making often with cutting turning his on which pershare pershare # earnings ### has myself then knew letter some b largest put duties approaching of wells words we were these pretax skeptical in our earnings forget thought we put managers a bundle mother a. has grandson governmental\n",
            "Worst case Perplexity: 776\n",
            "LSTM Perplexity: 35779.15214654369\n"
          ]
        }
      ]
    }
  ]
}