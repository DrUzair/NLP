{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The GAP (Gendered Ambiguous Pronouns) dataset\n",
        "\n",
        "The dataset is designed for coreference resolution tasks, specifically for resolving ambiguous pronouns to their correct antecedents. It contains English sentences with ambiguous pronouns and corresponding entities. The primary goal is to develop models that can correctly identify whether a given pronoun refers to \"A,\" \"B,\" or neither.\n",
        "\n",
        "Here's a brief summary of the structure of the GAP dataset:\n",
        "\n",
        "1. **Columns:**\n",
        "   - **ID:** A unique identifier for each example.\n",
        "   - **Text:** The text of the sentence containing the ambiguous pronoun.\n",
        "   - **Pronoun:** The ambiguous pronoun in the sentence.\n",
        "   - **Pronoun-offset:** The offset (position) of the pronoun in the sentence.\n",
        "   - **A, B:** The candidate entities to which the pronoun may refer.\n",
        "   - **A-offset, B-offset:** The offsets of entities A and B in the sentence.\n",
        "   - **A-coref, B-coref:** Binary labels indicating whether the pronoun refers to entities A or B.\n",
        "\n",
        "2. **Labels:**\n",
        "   - **A-coref, B-coref:** These binary labels are used for training the model. A label of 1 indicates that the pronoun refers to the corresponding entity, and 0 indicates it does not.\n",
        "\n",
        "3. **Task:**\n",
        "   - The task associated with this dataset is to build a model that, given a sentence with an ambiguous pronoun, predicts whether the pronoun refers to entity A, entity B, or neither.\n",
        "\n",
        "Here is a snippet of what the data might look like:\n",
        "\n",
        "```plaintext\n",
        "ID, Text, Pronoun, Pronoun-offset, A, A-offset, B, B-offset, A-coref, B-coref\n",
        "example1, \"John met Susan in the park. He said she had a dog.\", he, 35, John, 0, Susan, 16, True, False\n",
        "example2, \"Alice and Bob went to the store. They bought groceries.\", they, 35, Alice, 0, Bob, 11, True, False\n",
        "```\n",
        "\n",
        "In this example, the model needs to predict whether \"he\" refers to John or Susan and whether \"they\" refers to Alice or Bob.\n",
        "\n"
      ],
      "metadata": {
        "id": "BW6uuBLj07lD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The `CoRefModel`\n",
        "\n",
        "CoRefModelModel is a simple neural network model designed for pairwise ranking tasks, such as the task of ranking ambiguous pronoun candidates in coreference resolution. Let's break down the components and discuss their relevance to the task:\n",
        "\n",
        "```python\n",
        "class CoRefModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(CoRefModelModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "```\n",
        "\n",
        "1. **Initialization (`__init__` method):**\n",
        "   - `input_dim`: This parameter represents the dimensionality of the input features. In the case of coreference resolution, it could be the dimensionality of the feature vectors representing pairs of mentions (e.g., TF-IDF vectors or embeddings).\n",
        "\n",
        "   - `nn.Linear(input_dim, 64)`: This is the first fully connected (linear) layer. It takes the input features and maps them to a 64-dimensional intermediate representation.\n",
        "\n",
        "   - `nn.ReLU()`: The Rectified Linear Unit (ReLU) activation function is applied element-wise after the first linear layer. ReLU introduces non-linearity to the model, allowing it to learn complex relationships in the data.\n",
        "\n",
        "   - `nn.Linear(64, 1)`: The second linear layer reduces the 64-dimensional representation to a single output. In the context of CoRefModel, this output is interpreted as the predicted ranking score for a pair of mentions.\n",
        "\n",
        "```python\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "2. **Forward Pass (`forward` method):**\n",
        "   - `x`: This represents the input features, such as the TF-IDF vectors or embeddings for pairs of mentions.\n",
        "\n",
        "   - `self.relu(self.fc1(x))`: The input features pass through the first linear layer, followed by the ReLU activation function. This introduces non-linearity to the model's transformations.\n",
        "\n",
        "   - `self.fc2(x)`: The output of the first layer is then passed through the second linear layer, producing a single-dimensional output. In the context of pairwise ranking, this output can be interpreted as the predicted ranking score for the pair of mentions.\n",
        "\n",
        "   - `return x`: The final output is returned, representing the model's predicted ranking score for the input pair of mentions.\n",
        "\n",
        "**Relevance to the Task:**\n",
        "   - The model is designed for pairwise ranking, which is suitable for tasks where the goal is to rank pairs of items. In coreference resolution, this can be used to rank pairs of candidate antecedents for an ambiguous pronoun.\n",
        "\n",
        "   - The model architecture with two linear layers and a ReLU activation allows the network to capture complex relationships and patterns in the input data.\n",
        "\n",
        "   - The single-dimensional output from the model can be used to compare and rank pairs of mentions, aiding in the decision of whether a pronoun refers to one entity over another.\n",
        "\n",
        "   - The choice of activation functions and the architecture is common in neural network models for ranking tasks, providing a balance between expressiveness and simplicity.\n",
        "\n",
        "In summary, the `CoRefModelModel` is a neural network architecture tailored for the task of co-reference resolution, making it relevant for scenarios like coreference resolution where the goal is to rank potential antecedents for ambiguous pronouns."
      ],
      "metadata": {
        "id": "glW-U0L62CRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "Certainly! Let's break down the preprocessing steps:\n",
        "\n",
        "1. **Loading the Dataset:**\n",
        "   ```python\n",
        "   url = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\"\n",
        "   gap_data = pd.read_csv(url, sep='\\t')\n",
        "   ```\n",
        "   - The dataset is loaded from the provided URL using `pd.read_csv`. The `sep='\\t'` parameter indicates that the data is tab-separated.\n",
        "\n",
        "2. **Creating Pairs and Labels:**\n",
        "   ```python\n",
        "   pairs = []\n",
        "   labels = []\n",
        "\n",
        "   for index, row in gap_data.iterrows():\n",
        "       mention1 = row[\"Text\"]\n",
        "       mention2 = row[\"Pronoun\"]\n",
        "\n",
        "       # Assign label based on whether the pronoun refers to the same entity (1) or not (0)\n",
        "       label = 1 if row[\"A-coref\"] or row[\"B-coref\"] else 0\n",
        "\n",
        "       pairs.append({\"mention1\": mention1, \"mention2\": mention2})\n",
        "       labels.append(label)\n",
        "   ```\n",
        "   - For each row in the dataset, two mentions (`mention1` and `mention2`) are extracted from the columns \"Text\" and \"Pronoun.\"\n",
        "\n",
        "   - The label is assigned based on whether the pronoun refers to entity A or B (1) or neither (0).\n",
        "\n",
        "   - Pairs of mentions and their corresponding labels are stored in the `pairs` and `labels` lists.\n",
        "\n",
        "3. **Feature Engineering with TF-IDF:**\n",
        "   ```python\n",
        "   vectorizer = TfidfVectorizer()\n",
        "   features = vectorizer.fit_transform([pair[\"mention1\"] + \" \" + pair[\"mention2\"] for pair in pairs])\n",
        "   ```\n",
        "   - A `TfidfVectorizer` is used to convert pairs of mentions into TF-IDF (Term Frequency-Inverse Document Frequency) vectors.\n",
        "\n",
        "   - The TF-IDF vectors are computed based on the concatenation of `mention1` and `mention2` for each pair.\n",
        "\n",
        "4. **Converting to PyTorch Tensors:**\n",
        "   ```python\n",
        "   X = torch.tensor(features.toarray(), dtype=torch.float32)\n",
        "   y = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)\n",
        "   ```\n",
        "   - The TF-IDF vectors (`features`) are converted to a PyTorch tensor (`X`) with a data type of `torch.float32`.\n",
        "\n",
        "   - The labels (`labels`) are also converted to a PyTorch tensor (`y`) with the same data type. Additionally, `unsqueeze(1)` is used to convert the 1D tensor into a column vector, as CoRefModel expects labels in this format.\n",
        "\n",
        "The resulting `X` and `y` tensors can be used for training and evaluating the CoRefModel model on the pairwise ranking task."
      ],
      "metadata": {
        "id": "i2MdRbi722k_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "0rKXyRbEqpBw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CoRefModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(CoRefModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "aBrtDmD7FiBU"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GAP dataset from the URL\n",
        "url = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\"\n",
        "gap_data = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "# Preprocess data: create pairs of mentions and labels\n",
        "pairs = []\n",
        "labels = []\n",
        "\n",
        "for index, row in gap_data.iterrows():\n",
        "    mention1 = row[\"Text\"]\n",
        "    mention2 = row[\"Pronoun\"]\n",
        "\n",
        "    # Assign label based on whether the pronoun refers to the same entity (1) or not (0)\n",
        "    label = 1 if row[\"A-coref\"] or row[\"B-coref\"] else 0\n",
        "\n",
        "    pairs.append({\"mention1\": mention1, \"mention2\": mention2})\n",
        "    labels.append(label)\n"
      ],
      "metadata": {
        "id": "UBs036h0Dgcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Examples:\\n{pairs[:5]}\")\n",
        "print(len(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZpC23BKEVxr",
        "outputId": "8751f235-422f-484c-aa0a-eed05d9e0718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples:\n",
            "[{'mention1': \"Zoe Telford -- played the police officer girlfriend of Simon, Maggie. Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again. Phoebe Thomas played Cheryl Cassidy, Pauline's friend and also a year 11 pupil in Simon's class. Dumped her boyfriend following Simon's advice after he wouldn't have sex with her but later realised this was due to him catching crabs off her friend Pauline.\", 'mention2': 'her'}, {'mention1': 'He grew up in Evanston, Illinois the second oldest of five children including his brothers, Fred and Gordon and sisters, Marge (Peppy) and Marilyn. His high school days were spent at New Trier High School in Winnetka, Illinois. MacKenzie studied with Bernard Leach from 1949 to 1952. His simple, wheel-thrown functional pottery is heavily influenced by the oriental aesthetic of Shoji Hamada and Kanjiro Kawai.', 'mention2': 'His'}, {'mention1': \"He had been reelected to Congress, but resigned in 1990 to accept a post as Ambassador to Brazil. De la Sota again ran for governor of C*rdoba in 1991. Defeated by Governor Angeloz by over 15%, this latter setback was significant because it cost De la Sota much of his support within the Justicialist Party (which was flush with victory in the 1991 mid-terms), leading to President Carlos Menem 's endorsement of a separate party list in C*rdoba for the 1993 mid-term elections, and to De la Sota's failure to regain a seat in Congress.\", 'mention2': 'his'}, {'mention1': \"The current members of Crime have also performed in San Francisco under the band name ''Remote Viewers``. Strike has published two works of fiction in recent years: Ports of Hell, which is listed in the Rock and Roll Hall of Fame Library, and A Loud Humming Sound Came from Above. Rank has produced numerous films (under his real name, Henry Rosenthal) including the hit The Devil and Daniel Johnston.\", 'mention2': 'his'}, {'mention1': \"Her Santa Fe Opera debut in 2005 was as Nuria in the revised edition of Golijov's Ainadamar. She sang on the subsequent Deutsche Grammophon recording of the opera. For his opera Doctor Atomic, Adams rewrote the role of Kitty Oppenheimer, originally a mezzo-soprano role, for soprano voice, and Rivera sang the rewritten part of Kitty Oppenheimer at Lyric Opera of Chicago, De Nederlandse Opera, and the Metropolitan Opera., all in 2007. She has since sung several parts and roles in John Adams' works, including the soprano part in El Ni*o, and the role of Kumudha in A Flowering Tree in the Peter Sellars production at the New Crowned Hope Festival in Vienna.\", 'mention2': 'She'}]\n",
            "2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Featurization"
      ],
      "metadata": {
        "id": "5MSm_nkTDrv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Feature engineering: use TF-IDF vectors as features\n",
        "vectorizer = TfidfVectorizer()\n",
        "features = vectorizer.fit_transform([pair[\"mention1\"] + \" \" + pair[\"mention2\"] for pair in pairs])"
      ],
      "metadata": {
        "id": "9A4rHo97Dmj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV0Q6aBFEvFn",
        "outputId": "b5114614-4b15-4718-e78f-65b4d94cdb94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 20664)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Put to tensors"
      ],
      "metadata": {
        "id": "9aEIqf0mD3BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert features and labels to PyTorch tensors\n",
        "X = torch.tensor(features.toarray(), dtype=torch.float32)\n",
        "y = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)  # CoRefModel expects labels in column vector form"
      ],
      "metadata": {
        "id": "8VDuApQRDkn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train , test split"
      ],
      "metadata": {
        "id": "w6-WLWeXD-uL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "zCJ8PZKaEBgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"+ve Examples: {y_train.sum()/y_train.shape[0]}\")\n",
        "print(f\"input size: {X_train.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w0eJoyHFFH6",
        "outputId": "5f1997bc-d911-4528-fbbc-2e7fc1d4b744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+ve Examples: 0.8974999785423279\n",
            "input size: 20664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loader"
      ],
      "metadata": {
        "id": "D5XSKqBVKOD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define a custom dataset class\n",
        "class CoRefModeltDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.features[index], self.labels[index]\n"
      ],
      "metadata": {
        "id": "0O9C70tXKNCZ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset instance\n",
        "gap_dataset = CoRefModeltDataset(features=X_train, labels=y_train)\n"
      ],
      "metadata": {
        "id": "p-Ps4nHuKXA6"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "cSpON572EI0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 32  # Choose an appropriate batch size\n",
        "train_loader = DataLoader(dataset=gap_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "# Instantiate your CoRefModel, optimizer, and loss function\n",
        "model = CoRefModel(input_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Set the number of training epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop with DataLoader\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Iterate through batches in the DataLoader\n",
        "    for batch_inputs, batch_labels in train_loader:\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        batch_outputs = model(batch_inputs)\n",
        "        batch_loss = criterion(batch_outputs, batch_labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print or log the training loss at the end of each epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {batch_loss.item()}')\n",
        "\n",
        "# Training complete\n",
        "print('Training finished.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QkuJk_WJAGn",
        "outputId": "4adeae19-3072-4d43-82e6-93b1461532c2"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.4676911234855652\n",
            "Epoch [2/10], Loss: 0.2628365755081177\n",
            "Epoch [3/10], Loss: 0.15499475598335266\n",
            "Epoch [4/10], Loss: 0.33937108516693115\n",
            "Epoch [5/10], Loss: 0.16770993173122406\n",
            "Epoch [6/10], Loss: 0.0981973186135292\n",
            "Epoch [7/10], Loss: 0.041659265756607056\n",
            "Epoch [8/10], Loss: 0.12453542649745941\n",
            "Epoch [9/10], Loss: 0.029852237552404404\n",
            "Epoch [10/10], Loss: 0.024321584030985832\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "kTqBvbaRENDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "# Set a threshold for binary classification\n",
        "threshold = 0.5\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    test_predictions = (test_outputs > threshold).float()\n",
        "\n",
        "    # Convert to numpy arrays for Scikit-learn compatibility\n",
        "    y_test_np = y_test.numpy() if isinstance(y_test, torch.Tensor) else y_test\n",
        "    test_predictions_np = test_predictions.numpy()\n",
        "\n",
        "    # Calculate precision using Scikit-learn\n",
        "    precision = precision_score(y_test_np, test_predictions_np)\n",
        "    print(\"Precision:\", precision)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WoNhlriEPDn",
        "outputId": "5aa19ab5-a8ee-4329-babd-7da6bb180651"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9120603015075377\n"
          ]
        }
      ]
    }
  ]
}