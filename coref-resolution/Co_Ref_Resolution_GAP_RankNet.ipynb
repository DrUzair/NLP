{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The GAP (Gendered Ambiguous Pronouns) dataset\n",
        "\n",
        "The dataset is designed for coreference resolution tasks, specifically for resolving ambiguous pronouns to their correct antecedents. It contains English sentences with ambiguous pronouns and corresponding entities. The primary goal is to develop models that can correctly identify whether a given pronoun refers to \"A,\" \"B,\" or neither.\n",
        "\n",
        "Here's a brief summary of the structure of the GAP dataset:\n",
        "\n",
        "1. **Columns:**\n",
        "   - **ID:** A unique identifier for each example.\n",
        "   - **Text:** The text of the sentence containing the ambiguous pronoun.\n",
        "   - **Pronoun:** The ambiguous pronoun in the sentence.\n",
        "   - **Pronoun-offset:** The offset (position) of the pronoun in the sentence.\n",
        "   - **A, B:** The candidate entities to which the pronoun may refer.\n",
        "   - **A-offset, B-offset:** The offsets of entities A and B in the sentence.\n",
        "   - **A-coref, B-coref:** Binary labels indicating whether the pronoun refers to entities A or B.\n",
        "\n",
        "2. **Labels:**\n",
        "   - **A-coref, B-coref:** These binary labels are used for training the model. A label of 1 indicates that the pronoun refers to the corresponding entity, and 0 indicates it does not.\n",
        "\n",
        "3. **Task:**\n",
        "   - The task associated with this dataset is to build a model that, given a sentence with an ambiguous pronoun, predicts whether the pronoun refers to entity A, entity B, or neither.\n",
        "\n",
        "Here is a snippet of what the data might look like:\n",
        "\n",
        "```plaintext\n",
        "ID, Text, Pronoun, Pronoun-offset, A, A-offset, B, B-offset, A-coref, B-coref\n",
        "example1, \"John met Susan in the park. He said she had a dog.\", he, 35, John, 0, Susan, 16, True, False\n",
        "example2, \"Alice and Bob went to the store. They bought groceries.\", they, 35, Alice, 0, Bob, 11, True, False\n",
        "```\n",
        "\n",
        "In this example, the model needs to predict whether \"he\" refers to John or Susan and whether \"they\" refers to Alice or Bob.\n",
        "\n"
      ],
      "metadata": {
        "id": "BW6uuBLj07lD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The `RankNetModel`\n",
        "\n",
        "RankNetModel is a simple neural network model designed for pairwise ranking tasks, such as the task of ranking ambiguous pronoun candidates in coreference resolution. Let's break down the components and discuss their relevance to the task:\n",
        "\n",
        "```python\n",
        "class RankNetModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(RankNetModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "```\n",
        "\n",
        "1. **Initialization (`__init__` method):**\n",
        "   - `input_dim`: This parameter represents the dimensionality of the input features. In the case of coreference resolution, it could be the dimensionality of the feature vectors representing pairs of mentions (e.g., TF-IDF vectors or embeddings).\n",
        "\n",
        "   - `nn.Linear(input_dim, 64)`: This is the first fully connected (linear) layer. It takes the input features and maps them to a 64-dimensional intermediate representation.\n",
        "\n",
        "   - `nn.ReLU()`: The Rectified Linear Unit (ReLU) activation function is applied element-wise after the first linear layer. ReLU introduces non-linearity to the model, allowing it to learn complex relationships in the data.\n",
        "\n",
        "   - `nn.Linear(64, 1)`: The second linear layer reduces the 64-dimensional representation to a single output. In the context of RankNet, this output is interpreted as the predicted ranking score for a pair of mentions.\n",
        "\n",
        "```python\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "2. **Forward Pass (`forward` method):**\n",
        "   - `x`: This represents the input features, such as the TF-IDF vectors or embeddings for pairs of mentions.\n",
        "\n",
        "   - `self.relu(self.fc1(x))`: The input features pass through the first linear layer, followed by the ReLU activation function. This introduces non-linearity to the model's transformations.\n",
        "\n",
        "   - `self.fc2(x)`: The output of the first layer is then passed through the second linear layer, producing a single-dimensional output. In the context of pairwise ranking, this output can be interpreted as the predicted ranking score for the pair of mentions.\n",
        "\n",
        "   - `return x`: The final output is returned, representing the model's predicted ranking score for the input pair of mentions.\n",
        "\n",
        "**Relevance to the Task:**\n",
        "   - The model is designed for pairwise ranking, which is suitable for tasks where the goal is to rank pairs of items. In coreference resolution, this can be used to rank pairs of candidate antecedents for an ambiguous pronoun.\n",
        "\n",
        "   - The model architecture with two linear layers and a ReLU activation allows the network to capture complex relationships and patterns in the input data.\n",
        "\n",
        "   - The single-dimensional output from the model can be used to compare and rank pairs of mentions, aiding in the decision of whether a pronoun refers to one entity over another.\n",
        "\n",
        "   - The choice of activation functions and the architecture is common in neural network models for ranking tasks, providing a balance between expressiveness and simplicity.\n",
        "\n",
        "In summary, the `RankNetModel` is a neural network architecture tailored for the task of pairwise ranking, making it relevant for scenarios like coreference resolution where the goal is to rank potential antecedents for ambiguous pronouns."
      ],
      "metadata": {
        "id": "glW-U0L62CRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "Certainly! Let's break down the preprocessing steps:\n",
        "\n",
        "1. **Loading the Dataset:**\n",
        "   ```python\n",
        "   url = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\"\n",
        "   gap_data = pd.read_csv(url, sep='\\t')\n",
        "   ```\n",
        "   - The dataset is loaded from the provided URL using `pd.read_csv`. The `sep='\\t'` parameter indicates that the data is tab-separated.\n",
        "\n",
        "2. **Creating Pairs and Labels:**\n",
        "   ```python\n",
        "   pairs = []\n",
        "   labels = []\n",
        "\n",
        "   for index, row in gap_data.iterrows():\n",
        "       mention1 = row[\"Text\"]\n",
        "       mention2 = row[\"Pronoun\"]\n",
        "\n",
        "       # Assign label based on whether the pronoun refers to the same entity (1) or not (0)\n",
        "       label = 1 if row[\"A-coref\"] or row[\"B-coref\"] else 0\n",
        "\n",
        "       pairs.append({\"mention1\": mention1, \"mention2\": mention2})\n",
        "       labels.append(label)\n",
        "   ```\n",
        "   - For each row in the dataset, two mentions (`mention1` and `mention2`) are extracted from the columns \"Text\" and \"Pronoun.\"\n",
        "\n",
        "   - The label is assigned based on whether the pronoun refers to entity A or B (1) or neither (0).\n",
        "\n",
        "   - Pairs of mentions and their corresponding labels are stored in the `pairs` and `labels` lists.\n",
        "\n",
        "3. **Feature Engineering with TF-IDF:**\n",
        "   ```python\n",
        "   vectorizer = TfidfVectorizer()\n",
        "   features = vectorizer.fit_transform([pair[\"mention1\"] + \" \" + pair[\"mention2\"] for pair in pairs])\n",
        "   ```\n",
        "   - A `TfidfVectorizer` is used to convert pairs of mentions into TF-IDF (Term Frequency-Inverse Document Frequency) vectors.\n",
        "\n",
        "   - The TF-IDF vectors are computed based on the concatenation of `mention1` and `mention2` for each pair.\n",
        "\n",
        "4. **Converting to PyTorch Tensors:**\n",
        "   ```python\n",
        "   X = torch.tensor(features.toarray(), dtype=torch.float32)\n",
        "   y = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)\n",
        "   ```\n",
        "   - The TF-IDF vectors (`features`) are converted to a PyTorch tensor (`X`) with a data type of `torch.float32`.\n",
        "\n",
        "   - The labels (`labels`) are also converted to a PyTorch tensor (`y`) with the same data type. Additionally, `unsqueeze(1)` is used to convert the 1D tensor into a column vector, as RankNet expects labels in this format.\n",
        "\n",
        "The resulting `X` and `y` tensors can be used for training and evaluating the RankNet model on the pairwise ranking task."
      ],
      "metadata": {
        "id": "i2MdRbi722k_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0rKXyRbEqpBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3909fa28-28c0-4803-8452-ca40737b7bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0024999999441206455\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the RankNet model\n",
        "class RankNetModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(RankNetModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load GAP dataset from the URL\n",
        "url = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\"\n",
        "gap_data = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "# Preprocess data: create pairs of mentions and labels\n",
        "pairs = []\n",
        "labels = []\n",
        "\n",
        "for index, row in gap_data.iterrows():\n",
        "    mention1 = row[\"Text\"]\n",
        "    mention2 = row[\"Pronoun\"]\n",
        "\n",
        "    # Assign label based on whether the pronoun refers to the same entity (1) or not (0)\n",
        "    label = 1 if row[\"A-coref\"] or row[\"B-coref\"] else 0\n",
        "\n",
        "    pairs.append({\"mention1\": mention1, \"mention2\": mention2})\n",
        "    labels.append(label)\n",
        "\n",
        "# Feature engineering: use TF-IDF vectors as features\n",
        "vectorizer = TfidfVectorizer()\n",
        "features = vectorizer.fit_transform([pair[\"mention1\"] + \" \" + pair[\"mention2\"] for pair in pairs])\n",
        "\n",
        "# Convert features and labels to PyTorch tensors\n",
        "X = torch.tensor(features.toarray(), dtype=torch.float32)\n",
        "y = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)  # RankNet expects labels in column vector form\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize RankNet model\n",
        "input_dim = X_train.shape[1]\n",
        "model = RankNetModel(input_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy with Logits\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "\n",
        "    # Create pairwise labels\n",
        "    pairwise_labels = torch.sign(y_train.view(-1, 1) - y_train.view(1, -1))\n",
        "\n",
        "    # Reshape the model's output to match the pairwise labels and convert to float\n",
        "    outputs = outputs.view(-1, 1).expand(-1, len(y_train)).float()\n",
        "\n",
        "    # Compute the loss using the pairwise labels\n",
        "    loss = criterion(outputs, pairwise_labels)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    test_predictions = torch.argsort(test_outputs.view(-1))\n",
        "\n",
        "    precision = torch.sum((test_predictions == torch.arange(len(y_test))).float()) / len(y_test)\n",
        "    print(\"Precision:\", precision.item())\n"
      ]
    }
  ]
}