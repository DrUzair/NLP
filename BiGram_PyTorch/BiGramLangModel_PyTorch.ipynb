{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OEbaodEV9wx",
        "outputId": "9f3157b2-30c1-4933-c2b6-ab57d1ab5def"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZeoMDjvnSkQ",
        "outputId": "73f1ad73-0b69-4a93-cff1-5b5012bf8701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n",
            "{'he', 'chases', 'sleeps', 'leaps', 'brown', 'gone', 'waits', 'sleepy', 'dog', 'because', 'bright', 'over', 'to', 'fox', 'and', 'at', '.', ' ', 'barks', 'sees', 'calm', 'she', 'nearby', 'until', 'watches', 'see', 'as', 'quick', 'is', 'through', 'dashes', 'already', 'can', 'moon', 'remains', 'refuses', 'before', 'tree', 'who', 'move', 'slow', 'yawns', 'past', 'beside', 'quickly', 'river', 'rushes', 'under', 'than', 'him', 'small', 'runs', 'closes', 'splashes', ',', 'fence', 'stretches', 'his', 'while', 'but', 'teases', 'eyes', 'running', 'grass', 'lazy', 'ignores', 'circles', 'jumping', 'playing', 'naps', 'jumps', 'still', 'the', 'behind', 'resting', 'clever', 'sighs', 'a', 'leaving', 'too', 'higher'}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import requests\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, batch_size=4, input_length=8, train_iters=100, eval_iters=100):\n",
        "        super().__init__()\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # input_length = how many consecutive tokens/chars in one input\n",
        "        self.input_length = input_length\n",
        "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
        "        self.batch_size = batch_size\n",
        "        # train_iters = how many training iterations\n",
        "        self.train_iters= train_iters\n",
        "        # eval_iters = how many batches to evaluate to get average performance\n",
        "        self.eval_iters = eval_iters\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        logits = self.token_embeddings_table(inputs)\n",
        "        # print(logits.shape)\n",
        "        # logits are estimated model parameters\n",
        "        # for each input of context_size, there are vocab_size parameters to be estimated\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            batch_size, input_length, vocab_size = logits.shape\n",
        "            logits = logits.view(batch_size * input_length, vocab_size)\n",
        "            targets = targets.view(batch_size * input_length)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def fit(self, learning_rate=0.001):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        for iter in range(self.train_iters):\n",
        "            if iter % (self.train_iters//20) == 0:\n",
        "                avg_loss = self.eval_loss()\n",
        "                print(f\"iter {iter} train {avg_loss['train']} val {avg_loss['eval']}\")\n",
        "            inputs, targets = self.get_batch(split='train')\n",
        "            logits, loss = self(inputs, targets)\n",
        "            optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
        "            loss.backward()  # propagate loss back to the each unit in the network\n",
        "            optimizer.step()  # update network parameters w.r.t the loss\n",
        "\n",
        "        # print(loss.item())\n",
        "\n",
        "    def generate(self, context, max_new_tokens):\n",
        "        inputs = context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # forward pass, targets None, loss None\n",
        "            logits, _ = self(inputs)\n",
        "            # only last char/time-step is needed\n",
        "            logits = logits[:, -1, :]\n",
        "            # softmax logits to get probability distribution\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            # sample\n",
        "            sampled_output = torch.multinomial(probs, num_samples=1)\n",
        "            # append the sampled_output to running outputs\n",
        "            inputs = torch.cat((inputs, sampled_output), dim=1)\n",
        "        output_text = self.decoder(inputs[0].tolist())\n",
        "        return output_text\n",
        "\n",
        "    @torch.no_grad() # tell torch not to prepare for back-propagation\n",
        "    def eval_loss(self):\n",
        "        perf = {}\n",
        "        # set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "        self.eval()\n",
        "        for split in ['train', 'eval']:\n",
        "            losses = torch.zeros(self.eval_iters)\n",
        "            for k in range(self.eval_iters):\n",
        "                inputs, targets = self.get_batch(split)  # get random batch of inputs and targete\n",
        "                logits, loss = self(inputs, targets)  # forward pass\n",
        "                losses[k] = loss.item()  # the value of loss tensor as a standard Python number\n",
        "            perf[split] = losses.mean()\n",
        "        self.train() # turn-on training mode-\n",
        "        return perf\n",
        "\n",
        "    def prep(self, text):\n",
        "        vocab = sorted(list(set(text)))\n",
        "        self.vocab_size = len(vocab)\n",
        "        # look-up table for\n",
        "        self.token_embeddings_table = nn.Embedding(self.vocab_size, self.vocab_size)\n",
        "\n",
        "        ctoi = {c: i for i, c in enumerate(vocab)}  # char c to integer i map. assign value i for every word in vocab\n",
        "        itoc = {i: c for c, i in ctoi.items()}  # integer i to char c map\n",
        "\n",
        "        # print(ctoi)\n",
        "        # print(itoc)\n",
        "\n",
        "        self.encoder = lambda text: [ctoi[c] for c in text]\n",
        "        self.decoder = lambda nums: ''.join([itoc[i] for i in nums])\n",
        "\n",
        "        n = len(text)\n",
        "        self.train_text = text[:int(n * 0.9)]\n",
        "        self.val_text = text[int(n * 0.9):]\n",
        "\n",
        "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long)\n",
        "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long)\n",
        "\n",
        "    def prep_tokens(self, text):\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens.append(' ')\n",
        "        vocab = set(tokens)\n",
        "        print(vocab)\n",
        "        self.vocab_size = len(vocab)\n",
        "        # look-up table for\n",
        "        self.token_embeddings_table = nn.Embedding(self.vocab_size, self.vocab_size)\n",
        "\n",
        "        ctoi = {c: i for i, c in enumerate(vocab)}  # token c to integer i map. assign value i for every word in vocab\n",
        "        itoc = {i: c for c, i in ctoi.items()}  # integer i to token c map\n",
        "\n",
        "        # print(ctoi)\n",
        "        # print(itoc)\n",
        "\n",
        "        self.encoder = lambda text: [ctoi[c] for c in tokens]\n",
        "        self.decoder = lambda nums: ' '.join([itoc[i] for i in nums])\n",
        "\n",
        "        n = len(tokens)\n",
        "        self.train_text = tokens[:int(n * 0.9)]\n",
        "        self.val_text = tokens[int(n * 0.9):]\n",
        "\n",
        "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long)\n",
        "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long)\n",
        "\n",
        "    def get_batch(self, split='train'):\n",
        "        data = self.train_data if split == 'train' else self.val_data\n",
        "        ix = torch.randint(len(data) - self.input_length,\n",
        "                           (self.batch_size,))  # get random chunks of length batch_size from data\n",
        "        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])\n",
        "        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])\n",
        "        inputs_batch = inputs_batch.to(self.device)  # deploy to GPU is available\n",
        "        targets_batch = targets_batch.to(self.device)# deploy to GPU is available\n",
        "        return inputs_batch, targets_batch\n",
        "\n",
        "def fetch_text_from_url(url):\n",
        "    \"\"\"Fetches raw text from a given URL.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"  # Example: Pride and Prejudice\n",
        "text = fetch_text_from_url(url)\n",
        "\n",
        "text = \"\"\"a quick brown fox jumps over the lazy dog.\n",
        "          lazy dog and a quick brown fox.\n",
        "          the dog is lazy and the fox jumps quickly.\n",
        "          a fox jumps over the dog because he is lazy.\n",
        "          dog is lazy and fox is brown. she quickly jumps over the lazy dog.\n",
        "          the brown fox watches the lazy dog before jumping.\n",
        "          a lazy dog sleeps under the tree while the fox waits.\n",
        "          the quick fox sees the dog resting and leaps past him.\n",
        "          a small fox chases the dog, but he is too slow.\n",
        "          the dog barks at the fox, but she is already gone.\n",
        "          over the fence, the fox jumps while the dog sighs.\n",
        "          a sleepy dog ignores the fox playing nearby.\n",
        "          the fox teases the lazy dog, who refuses to move.\n",
        "          under the bright moon, the fox runs and the dog yawns.\n",
        "          the brown fox leaps higher than the sleepy dog can see.\n",
        "          beside the river, the lazy dog naps as the fox splashes.\n",
        "          a clever fox waits until the dog closes his eyes before running.\n",
        "          the dog stretches and yawns while the fox rushes past.\n",
        "          the fox circles the dog, but he remains still and calm.\n",
        "          a quick fox dashes through the grass, leaving the lazy dog behind.\n",
        "          \"\"\"\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "model = BigramLanguageModel(batch_size=32,\n",
        "                            input_length=8,\n",
        "                            train_iters=5000)\n",
        "model = model.to(model.device)\n",
        "model.prep_tokens(text)\n",
        "input_batch, output_batch = model.get_batch(split='train')\n",
        "# print(input_batch)\n",
        "# print(output_batch)\n",
        "\n",
        "logits, loss = model(input_batch, output_batch)\n",
        "# print(logits.shape)\n",
        "# print(logits)\n",
        "# print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "outputs = model.generate(context=torch.zeros((1, 1), dtype=torch.long, device=model.device),\n",
        "                         max_new_tokens=100)\n",
        "print(outputs)\n",
        "print(f\"Vocab size {model.vocab_size}, CE: {-np.log(1/model.vocab_size)}\")\n",
        "model.fit(learning_rate=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9SC1I_hTdXC",
        "outputId": "7778cda9-d887-4869-9e9f-eab738e6325b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he is sleepy river . see leaving because sleeps quickly , before bright the dog slow bright quickly at . rushes behind , waits waits eyes ignores closes until yawns until yawns the leaps a tree calm leaps sees stretches grass through grass   leaving brown see leaving already a stretches who see stretches behind waits at dashes , move she jumps yawns over leaving circles chases gone fence he to under at dashes bright remains   who at because slow higher who refuses fence gone higher over bright leaps fox behind and chases too still moon closes while running at\n",
            "Vocab size 81, CE: 4.394449154672439\n",
            "iter 0 train 4.77914571762085 val 4.7747883796691895\n",
            "iter 250 train 1.2424339056015015 val 1.2333190441131592\n",
            "iter 500 train 1.2285927534103394 val 1.222877860069275\n",
            "iter 750 train 1.2245844602584839 val 1.221291422843933\n",
            "iter 1000 train 1.2233067750930786 val 1.2344472408294678\n",
            "iter 1250 train 1.2214137315750122 val 1.2291940450668335\n",
            "iter 1500 train 1.2301043272018433 val 1.221910834312439\n",
            "iter 1750 train 1.222995400428772 val 1.2235888242721558\n",
            "iter 2000 train 1.2265223264694214 val 1.229246973991394\n",
            "iter 2250 train 1.2206987142562866 val 1.2188712358474731\n",
            "iter 2500 train 1.2205092906951904 val 1.2124449014663696\n",
            "iter 2750 train 1.2238571643829346 val 1.227217435836792\n",
            "iter 3000 train 1.2209974527359009 val 1.224136233329773\n",
            "iter 3250 train 1.225286602973938 val 1.2296890020370483\n",
            "iter 3500 train 1.2261613607406616 val 1.2320464849472046\n",
            "iter 3750 train 1.2211564779281616 val 1.2284184694290161\n",
            "iter 4000 train 1.2198904752731323 val 1.218097448348999\n",
            "iter 4250 train 1.2211307287216187 val 1.2261182069778442\n",
            "iter 4500 train 1.2124987840652466 val 1.2242398262023926\n",
            "iter 4750 train 1.2320199012756348 val 1.2270750999450684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(context=torch.zeros((1, 1), dtype=torch.long, device=model.device), max_new_tokens=100)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq-N3SQ32U1q",
        "outputId": "c0b09391-102e-43c7-a5ff-68adf4d3ba24"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he is brown fox rushes past . the fox watches the dog naps as the brown fox is already gone . dog can see . the dog ignores the grass , but he is lazy and the fox waits until the sleepy dog is lazy dog is too slow . the fox waits . the fox waits until the brown fox sees the dog because he is too slow . she quickly . beside the grass , leaving the brown . a quick brown fox jumps over the fox rushes past . the sleepy dog barks at the grass , but\n"
          ]
        }
      ]
    }
  ]
}